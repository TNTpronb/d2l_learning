# 线性回归

## 1. 核心定义
线性回归是一种用于预测**连续数值**的监督学习模型。它假设特征（输入）与目标（输出）之间存在线性关系。

---

## 2. 模型形式推导

### 2.1 标量输入，标量输出
这是最直观的线性关系，只有一个特征 $x$ 和一个目标值 $y$。
$$ y = wx + b $$
*   **$w$**：权重（斜率），代表特征对目标的影响程度。
*   **$b$**：偏置（截距），代表当特征为 0 时的基础输出值。

### 2.2 向量输入，标量输出（多元线性回归）
当存在多个特征时，输入变为向量 $\mathbf{x}$，输出仍为标量 $y$。
#### 展开形式
$$ y = w_1x_1 + w_2x_2 + \dots + w_nx_n + b $$

#### 向量形式
为了统一表达形式，我们将偏置 $b$ 合并入权重向量中，并引入虚拟特征 $x_0 = 1$ 。
$$
\mathbf{x} = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} b \\ w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}
$$
**模型公式：**
$$ y = \mathbf{w}^\top \mathbf{x} $$

### 2.3 向量输入，向量输出（多输出线性回归）
当需要同时预测多个目标值时，输入为特征向量 $\mathbf{x}$，输出为目标向量 $\mathbf{y}$。
#### 展开形式
$$
\begin{aligned}
y_1 &= w_{11}x_1 + w_{12}x_2 + \dots + b_1 \\
y_2 &= w_{21}x_1 + w_{22}x_2 + \dots + b_2 \\
&\vdots \\
y_k &= w_{k1}x_1 + w_{k2}x_2 + \dots + b_k
\end{aligned}
$$

#### 矩阵形式
将偏置并入权重矩阵 $\mathbf{W}$。
$$
\mathbf{x} = \begin{bmatrix} 1 \\ x_1 \\ \vdots \\ x_n \end{bmatrix}, \quad
\mathbf{W} = \begin{bmatrix}
b_1 & b_2 & \dots & b_k \\
w_{11} & w_{21} & \dots & w_{k1} \\
\vdots & \vdots & \ddots & \vdots \\
w_{1n} & w_{2n} & \dots & w_{kn}
\end{bmatrix}
$$
**模型公式：**
$$ \mathbf{y} = \mathbf{x}^\top \mathbf{W} $$

---

## 3. 批量样本的矩阵表示
如果处理 $m$ 个训练样本，我们可以将所有数据整合成矩阵进行批量计算。

*   **特征矩阵** $\mathbf{X}$ (维度：$m \times (n+1)$)
*   **目标矩阵** $\mathbf{Y}$ (维度：$m \times k$)

**通用模型：**
$$ \mathbf{Y} = \mathbf{X} \mathbf{W} $$
*(注：此公式同时涵盖了单输出和多输出场景，是工程实现中最常用的形式)*

---

## 4. 损失函数 (Loss Function)
线性回归的目标是最小化**均方误差 (MSE)**，即预测值与真实值之间的平方误差均值。

### 单输出场景 (MSE)
$$ J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)})^2 $$

在论文理论论证场景中，也常用：

$$\mathcal{\ell}(\mathbf{X}, \mathbf{y}, \mathbf{w}, b) = \frac{1}{2n} \sum_{i=1}^{n} \left( y_i - \langle \mathbf{x}_i, \mathbf{w} \rangle - b \right)^2 = \frac{1}{2n} \left\| \mathbf{y} - \mathbf{X}\mathbf{w} - b \right\|^2
$$

### 多输出场景 (MSE)
$$ J(\mathbf{W}) = \frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{k} (y_j^{(i)} - \hat{y}_j^{(i)})^2 $$

---

## 5. 模型求解
通常通过以下两种方法求得最优权重 $\mathbf{w}$ 或 $\mathbf{W}$：

1. **正规方程 (Normal Equation)**
    $$ \mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} $$
2. **梯度下降 (Gradient Descent)**
    通过迭代更新权重，沿损失函数梯度下降方向寻找最优解。

---

## 6. 关键假设
为了保证模型有效性，线性回归通常假设：
*   **线性性**：特征与目标之间存在线性关系。
*   **独立性**：样本之间相互独立。
*   **正态性**：误差项（噪声）服从均值为 0 的正态分布。
*   **同方差性**：误差项的方差恒定。


# 线性回归显式解（正规方程）推导
> 核心目标：通过**矩阵求导**与**凸优化**，推导出线性回归损失函数最小化时的权重显式解 $\mathbf{w}^*$，全程基于「偏置并入权重」的统一形式。

## 1. 前置准备：统一模型形式（合并偏置项）
为简化推导，将线性回归的**偏置 $b$** 与**权重 $w$** 合并，消除独立的偏置参数。

### 1.1 符号定义
| 符号 | 维度 | 说明 |
|------|------|------|
| 原始特征矩阵 | $\mathbf{X}_{raw} \in \mathbf{R}^{n \times d}$ | $n$ 个样本，每个样本 $d$ 个特征 |
| 扩展特征矩阵 | $\mathbf{X} \in \mathbf{R}^{n \times (d+1)}$ | 拼接一列全1向量，即 $\mathbf{X} = [\mathbf{X}_{raw}, \mathbf{1}_n]$ |
| 原始权重向量 | $\mathbf{w}_{raw} \in \mathbf{R}^{d}$ | 仅包含特征权重 |
| 扩展权重向量 | $\mathbf{w} \in \mathbf{R}^{(d+1)}$ | 合并偏置，即 $\mathbf{w} = \begin{bmatrix} \mathbf{w}_{raw} \\ b \end{bmatrix}$ |
| 真实值向量 | $\mathbf{y} \in \mathbf{R}^n$ | 所有样本的真实标签 |
| 预测值向量 | $\hat{\mathbf{y}} \in \mathbf{R}^n$ | 模型预测值，$\hat{\mathbf{y}} = \mathbf{X}\mathbf{w}$ |

### 1.2 模型简化结果
合并后，线性回归的预测公式从 $y = \mathbf{x}^\top \mathbf{w}_{raw} + b$ 简化为：
$$\hat{\mathbf{y}} = \mathbf{X}\mathbf{w}$$

## 2. 损失函数定义
采用**带 $\frac{1}{2}$ 系数的均方误差（MSE）** 作为损失函数（$\frac{1}{2}$ 仅为简化求导，不改变最优解位置）：
$$\ell(\mathbf{X}, \mathbf{y}, \mathbf{w}) = \frac{1}{2n} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$$

### 损失函数展开（矩阵形式）
向量的二范数平方等于其自身的转置乘积，因此展开为：
$$\ell(\mathbf{w}) = \frac{1}{2n} (\mathbf{y} - \mathbf{X}\mathbf{w})^\top (\mathbf{y} - \mathbf{X}\mathbf{w})$$

## 3. 核心步骤：对权重 $\mathbf{w}$ 求梯度
损失函数是**凸函数**，其全局最小值点满足「梯度为0」。需通过**矩阵求导法则**计算 $\frac{\partial \ell}{\partial \mathbf{w}}$。

### 3.1 必备矩阵求导公式（核心）
设 $\mathbf{A}$ 为常数矩阵，$\mathbf{x}$ 为变量向量，有：
1. $\frac{\partial (\mathbf{A}\mathbf{x})}{\partial \mathbf{x}} = \mathbf{A}^\top$
2. $\frac{\partial (\mathbf{x}^\top \mathbf{A}\mathbf{x})}{\partial \mathbf{x}} = 2\mathbf{A}^\top\mathbf{x}$（$\mathbf{A}$ 为对称矩阵时，可简化为 $2\mathbf{A}\mathbf{x}$）

### 3.2 梯度推导过程
令 $\mathbf{z} = \mathbf{y} - \mathbf{X}\mathbf{w}$，则损失函数可表示为 $\ell(\mathbf{w}) = \frac{1}{2n} \mathbf{z}^\top \mathbf{z}$。

#### 步骤1：对 $\mathbf{z}^\top \mathbf{z}$ 求导
$$\frac{\partial (\mathbf{z}^\top \mathbf{z})}{\partial \mathbf{w}} = \frac{\partial (\mathbf{z}^\top \mathbf{z})}{\partial \mathbf{z}} \cdot \frac{\partial \mathbf{z}}{\partial \mathbf{w}}$$
- 由求导公式：$\frac{\partial (\mathbf{z}^\top \mathbf{z})}{\partial \mathbf{z}} = 2\mathbf{z}^\top$
- 计算 $\frac{\partial \mathbf{z}}{\partial \mathbf{w}}$：$\mathbf{z} = \mathbf{y} - \mathbf{X}\mathbf{w}$，故 $\frac{\partial \mathbf{z}}{\partial \mathbf{w}} = -\mathbf{X}$

#### 步骤2：代入并化简
$$\frac{\partial (\mathbf{z}^\top \mathbf{z})}{\partial \mathbf{w}} = 2\mathbf{z}^\top \cdot (-\mathbf{X}) = -2(\mathbf{y} - \mathbf{X}\mathbf{w})^\top \mathbf{X}$$

#### 步骤3：结合损失函数的系数
$$\frac{\partial \ell}{\partial \mathbf{w}} = \frac{1}{2n} \cdot \left[ -2(\mathbf{y} - \mathbf{X}\mathbf{w})^\top \mathbf{X} \right]$$
消去系数 $2$，最终梯度为：
$$\frac{\partial \ell}{\partial \mathbf{w}} = \frac{1}{n} (\mathbf{y} - \mathbf{X}\mathbf{w})^\top \mathbf{X}$$

> 补充等价形式：将转置符号调整后，可写为 $\frac{1}{n} \mathbf{X}^\top (\mathbf{X}\mathbf{w} - \mathbf{y})$，两者本质一致（标量的转置等于自身）。

## 4. 求解最优权重 $\mathbf{w}^*$
### 4.1 令梯度为0（极值条件）
因损失函数是凸函数，**梯度为0的点即为全局最优解**，因此令：
$$\frac{1}{n} (\mathbf{y} - \mathbf{X}\mathbf{w})^\top \mathbf{X} = \mathbf{0}$$

### 4.2 等式变形（核心推导）
1. 两边同乘 $n$（不改变等式成立性）：
   $$(\mathbf{y} - \mathbf{X}\mathbf{w})^\top \mathbf{X} = \mathbf{0}$$
2. 两边同时转置（等式仍成立）：
   $$\mathbf{X}^\top (\mathbf{y} - \mathbf{X}\mathbf{w}) = \mathbf{0}$$
3. 展开括号：
   $$\mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X}\mathbf{w} = \mathbf{0}$$
4. 移项整理：
   $$\mathbf{X}^\top \mathbf{X}\mathbf{w} = \mathbf{X}^\top \mathbf{y}$$

### 4.3 最终显式解
若矩阵 $\mathbf{X}^\top \mathbf{X}$ **可逆**（即特征矩阵无多重共线性、列满秩），两边同时左乘 $(\mathbf{X}^\top \mathbf{X})^{-1}$，得到最优权重：
$$\mathbf{w}^* = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}$$

此公式即为线性回归的**显式解**，也被称为**正规方程（Normal Equation）**。

## 5. 关键注意事项
1. **可逆性条件**：仅当 $\mathbf{X}^\top \mathbf{X}$ 可逆时，显式解存在。若不可逆（如特征冗余、样本数小于特征数），需通过**岭回归**（加正则项）或**奇异值分解（SVD）** 求解。
2. **计算效率**：显式解适合**小样本、低维度**场景；当特征维度极高（如 $d > 10^4$）时，矩阵求逆的时间复杂度为 $O(d^3)$，效率远低于梯度下降等迭代方法。
3. **偏置的还原**：求得的 $\mathbf{w}^*$ 最后一个元素即为偏置 $b$，前 $d$ 个元素为特征权重 $\mathbf{w}_{raw}$。