# 线性回归

## 1. 核心定义
线性回归是一种用于预测**连续数值**的监督学习模型。它假设特征（输入）与目标（输出）之间存在线性关系。

---

## 2. 模型形式推导

### 2.1 标量输入，标量输出
这是最直观的线性关系，只有一个特征 $x$ 和一个目标值 $y$。
$$ y = wx + b $$
*   **$w$**：权重（斜率），代表特征对目标的影响程度。
*   **$b$**：偏置（截距），代表当特征为 0 时的基础输出值。

### 2.2 向量输入，标量输出（多元线性回归）
当存在多个特征时，输入变为向量 $\mathbf{x}$，输出仍为标量 $y$。
#### 展开形式
$$ y = w_1x_1 + w_2x_2 + \dots + w_nx_n + b $$

#### 向量形式
为了统一表达形式，我们将偏置 $b$ 合并入权重向量中，并引入虚拟特征 $x_0 = 1$ 。
$$
\mathbf{x} = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \quad
\mathbf{w} = \begin{bmatrix} b \\ w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}
$$
**模型公式：**
$$ y = \mathbf{w}^\top \mathbf{x} $$

### 2.3 向量输入，向量输出（多输出线性回归）
当需要同时预测多个目标值时，输入为特征向量 $\mathbf{x}$，输出为目标向量 $\mathbf{y}$。
#### 展开形式
$$
\begin{aligned}
y_1 &= w_{11}x_1 + w_{12}x_2 + \dots + b_1 \\
y_2 &= w_{21}x_1 + w_{22}x_2 + \dots + b_2 \\
&\vdots \\
y_k &= w_{k1}x_1 + w_{k2}x_2 + \dots + b_k
\end{aligned}
$$

#### 矩阵形式
将偏置并入权重矩阵 $\mathbf{W}$。
$$
\mathbf{x} = \begin{bmatrix} 1 \\ x_1 \\ \vdots \\ x_n \end{bmatrix}, \quad
\mathbf{W} = \begin{bmatrix}
b_1 & b_2 & \dots & b_k \\
w_{11} & w_{21} & \dots & w_{k1} \\
\vdots & \vdots & \ddots & \vdots \\
w_{1n} & w_{2n} & \dots & w_{kn}
\end{bmatrix}
$$
**模型公式：**
$$ \mathbf{y} = \mathbf{x}^\top \mathbf{W} $$

---

## 3. 批量样本的矩阵表示
如果处理 $m$ 个训练样本，我们可以将所有数据整合成矩阵进行批量计算。

*   **特征矩阵** $\mathbf{X}$ (维度：$m \times (n+1)$)
*   **目标矩阵** $\mathbf{Y}$ (维度：$m \times k$)

**通用模型：**
$$ \mathbf{Y} = \mathbf{X} \mathbf{W} $$
*(注：此公式同时涵盖了单输出和多输出场景，是工程实现中最常用的形式)*

---

## 4. 损失函数 (Loss Function)
线性回归的目标是最小化**均方误差 (MSE)**，即预测值与真实值之间的平方误差均值。

### 单输出场景 (MSE)
$$ J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)})^2 $$

在论文理论论证场景中，也常用：

$$\mathcal{\ell}(\mathbf{X}, \mathbf{y}, \mathbf{w}, b) = \frac{1}{2n} \sum_{i=1}^{n} \left( y_i - \langle \mathbf{x}_i, \mathbf{w} \rangle - b \right)^2 = \frac{1}{2n} \left\| \mathbf{y} - \mathbf{X}\mathbf{w} - b \right\|^2
$$

### 多输出场景 (MSE)
$$ J(\mathbf{W}) = \frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{k} (y_j^{(i)} - \hat{y}_j^{(i)})^2 $$

---

## 5. 模型求解
通常通过以下两种方法求得最优权重 $\mathbf{w}$ 或 $\mathbf{W}$：

1. **正规方程 (Normal Equation)**
    $$ \mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} $$
2. **梯度下降 (Gradient Descent)**
    通过迭代更新权重，沿损失函数梯度下降方向寻找最优解。

---

## 6. 关键假设
为了保证模型有效性，线性回归通常假设：
*   **线性性**：特征与目标之间存在线性关系。
*   **独立性**：样本之间相互独立。
*   **正态性**：误差项（噪声）服从均值为 0 的正态分布。
*   **同方差性**：误差项的方差恒定。